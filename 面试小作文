您好，面试官，我叫陈恒韬，本科毕业于广州航海学院，毕业后当过大概2年linux运维工程师，和大概4年的ETL工程师。
在做ETL工程师期间，经历过3个数仓项目，在数仓项目里负责的工作包括项目相关业务调研、ETL整体流程的设计，调度和监控、
对数据进行清洗、转换，编写日常工作所需的shell脚本，项目文档的编写。
在做好本职工作之上，还参与协助数仓模型的设计，报表的制作等，能够与业务员、架构师以及其他技术员进行有效的沟通和协作，保证项目工作能够高效展开实施。

最近一份工作是外包到泉州银行的零售贷款数仓项目。
我在该项目主要负责的内容有
1.客户在jira中把需求发送到我们的团队，然后我们会跟相关的业务员开需求澄清会议来确认取数逻辑，指标的粒度和口径
并且还会讨论如何实现需求，进行可行性分析，要用到哪些系统的业务表和字段，能否复用在数仓里已存在的表，并且设计好模型表等等；
2.我们的数仓是基于星型模型来设计的，每个开发人员都按照自己分配到的二级主题域来对DW层和集市层做数据的流转
3.项目里我主要负责的是风控域这个主题，从dwd层我主要做的事是根据指标体系认证书里面的口径，量化原子指标，确定清洗规则，
对ODS层数据进行去重，去空，去异常，统一数据类型，统一数据格式等等，然后把清洗好的数据加载到各个事实表里头，
之后再把贷款风控域主题相关的事实表和维表聚合成大宽表（零售信贷风控域基表），为集市层提供统一的基础数据和指标，
最后再根据需求用宽表层的基础数据作进一步的聚合，进而形成最终的指标并输出到各个集市表里，比如授信监控集市，贷款逾期集市，贷款申请集市等等；
4.完成sql代码后我会进行个自测，来对数据的唯一性，完整性，合法性，精确性，时效性, 依赖性作检查，一定程度上保证数据的质量，没问题后就把sql代码封装到shell脚本里，
并交给测试人员去做冒烟测试跟联调测试，测试通过后再去写上线文档并交给DBA去安排上线；

对这些贴源层新增数据作清洗转换，具体要清洗内容有：
去异常：比如贷款申请表里有异常申请的，应该去掉，
去重，比如关联多个系统里的表时可能有相同或类似口径的的数据，这种情况只需要其中一条即可
去空：比如风险分类记录表里，因为有些贷款是首次参加风险分类的，所以这些记录里第二次分类为空，处理方式是用nvl函数将空值转换成固定标签+随机数，
统一数据格式：像日期有用斜杠做年月日之间的分隔的，我们会把这种日期用regexp_replace函数匹配出来并替换成年月日连续8位数字等，
统一数据类型：我做过的表里像贷款合同表里的有些日期用的是date，像放款日期，有些用的是varchar2，比如还款日期，处理方式是用cast函数统一把date转成字符串类型。


数据来源的系统有核心系统，ECIF系统，零售信贷系统，票据系统等；


hive拉链表：https://blog.csdn.net/weixin_46389691/article/details/127973319
先对历史表跟增量表进行左连接，在历史表中作if判断，如果增量表没有或已经是历史数据的，就做保留，
增量表有且历史表是有效记录的，就对历史表作更新，将历史表原数据的有效时间改成系统时间的上一天，
这个过程就完成对历史拉链表的更新，最后再union all增量表就可以生成新的历史拉链表了

select xxx
  date_add(current_date(), -1) as start_time, 
  '9999-12-31' as end_time from ods.update_zip -- 增量表
union all -- 合并历史表
select a.xxx
  a.start_time, 
  if(b.id is null or a.end_time < '9999-12-31',a.end_time,date_sub(b.start_time,1)) end_time   # 不需要更新的数据（b.id为空的行）和 已经是历史数据的行（a.end_time < '9999-12-31'），end_time不变，依然用a.endtime，否则endtime改为上一天
from
  dwd.dwd_zip a -- 历史拉链表
left join
  ods.update_zip b -- 增量数据表
on
  a.id = b.id;


拉链表的数据进行回滚，恢复到 指定日期 rollback_date（2020-11-21） 那一天的数据
DROP TABLE IF EXISTS tmp.shops_tmp;
CREATE TABLE IF NOT EXISTS tmp.tmp_shops
SELECT *
       startdate,
       enddate
FROM dim.dim_trade_shops
WHERE enddate < '2020-11-21'         # end_date < rollback_date，即结束日期 < 回滚日期。表示该行数据在 rollback_date 之前产生，这些数据需要原样保留。
UNION ALL
SELECT *
       startdate,
       '9999-12-31' AS enddate
FROM dim.dim_trade_shops
WHERE startdate <= '2020-11-21' AND enddate >= '2020-11-21';       # start_date <= rollback_date <= end_date，即开始日期 <= 回滚日期 <= 结束日期。这些数据是回滚日期之后产生的，但是需要修改。将end_date 改为 9999-12-31

INSERT OVERWRITE TABLE dim.dim_trade_shops
SELECT *
FROM tmp.tmp_shops;


输出文档：mapping文档，模型文档，概要文档，详细文档，测试文档（sit，uat），上线文档

人员：共7人，1 项目经理，1 需求分析师，4 ETL工程师，1 测试工程师

每日总数据量：600万行
总数据量：10亿多行
单表单日新增数据量：80w行
最大单表数据量：1.5亿行
客户量：380w

银行系统：
BBS：票据系统	
CBS：核心系统
CCP：信用卡系统
CCS：对公信贷管理系统
CIF：ECIF系统
ERP：财务管理系统
HRM：人力资源管理系统
MCS：小微贷款系统
NBS：网上银行系统
PCS：零售信贷系统


主题：客户，银行卡，存款，交易，贷款，渠道，中间业务，总账

信贷主题域：用户域，授信域，流量域，交易域，展业域，风控域，催收域，公共主题等

贷款流程：注册--ocr认证---md5认证----借贷申请---信用资料---初审---终审---结果是否通过（否，提请复议---复议审核---是否通过)
          ---产生额度---通知客户---额度匹配--提款请求---提款审核---是否通过---签订协议
	  ---放款---还款---是否逾期---是否结清--结清--结束
		  
贷前：注册平台且提交申请资料，ocr认证、md5认证，审批
贷中：客户提出提款申请，进入审核，通过签订协议；
贷后：围绕平台放款，还款工作；


贷款主题
业务表：客户信息表，授信申请表，授信额度表，授信审批信息表，贷款申请表，贷款审批信息表，贷款账户交易表，贷款账户主表，贷款账户还款计划表，贷款账户还款信息表，贷款借据信息表，贷款合同信息表，贷款核销信息表

事实表：贷款账户放款明细表，贷款账户还款处理明细表，贷款交易明细表，贷款核销明细表
维度表：贷款逾期维表，贷款产品维表，客户维度表，抵押物维表，质押物维表

1.1.贷款申请通过率监控报表
表：客户信息表，贷款申请表，贷款审核表，贷款通过表

1）作用：监控申请通过率和申请的额度情况
2）指标及计算口径：
       申请量：突然起量时，要尤其关注是否存在欺诈风险。量萎缩明显时，也要关注原因，以免对利润产生影响
       通过率：通过数量 / 申请数量
       平均贷款额：总贷款额 / 通过人数
3）维度：申请时间(天/周/月)，认证方式，用户类型
5）分析角度：过低则说明客群质量不佳，过高则存在包装申请资料的风险


1.2.贷款五级分类报表
表：还款表：还款流水号, 贷款借据号, 还款方式，还款渠道，币种，还款周期，还款账号，总期数，本期期数，计息总期数，计息期数，贷款余额
    贷款借据信息表：贷款借据号，贷款账号，客户号，贷款合同号，总期数，当前已还款期数，每期还款额，贷款金额，贷款余额，执行利率，五级分类，逾期本金金额，当前逾期期数，逾期天数，放款账号，还款账号，还款间隔，贷款损失准备金额，最后更新时间
    贷款合同信息表：贷款合同号，贷款账号，贷款种类，贷款总金额，贷款余额，贷款期限，放款日期，放款账号，还款日，还款间隔，还款账号，执行利率，抵押物大类，担保方式，当前逾期期数，逾期金额
	
    贷款核销信息：贷款借据号，核销本金，核销利息，开始时间
    
统计粒度：以一个贷款借据号为一个记录

指标： 
      贷款申请通过率 = 当年贷款通过数量 / 当年贷款申请数量
	  （贷款申请表，贷款审批信息表）
	  
	  提现率 = 当年内授信后90天内申请提现用户数 / 当年内授信成功用户数
      （（授信审批信息表，筛选审批成功的客户） join （贷款申请表，客户ID去重） on a.客户ID = b.客户ID）
	  
	  
	  贷款拨备率 = 贷款损失准备金 / 总贷款余额 （这个指标反应了商业银行整体的拨备计提水平，是衡量银行抵御金融风险能力的重要指标之一）
	  
	  拨备覆盖率 = 贷款损失准备金 / 总不良贷款 （这个指标主要用于衡量银行对不良贷款的风险准备是否充足）
	                                       （贷款拨备率侧重于反映银行为应对贷款损失而准备的资金充足程度，而拨备覆盖率则侧重于评估银行是否充分计提了贷款损失准备金，以应对不良贷款的风险）
      
	  逾期贷款率 = 逾期贷款余额/各项贷款余额 （侧重反映整体的贷款偿还情况）
	   
	  不良贷款率 = 不良贷款余额/各项贷款余额 =（次级类贷款余额+可疑类贷款余额+损失类贷款余额）/各项贷款余额。（监管要求警戒线2%）
      
	  不良贷款生成率 = (本期不良贷款余额 - 上期不良贷款余额 + 本期核销的不良贷款) / 上期贷款余额
	  
	  
	  贷款增长率 = （本期贷款余额 - 上期贷款余额） / 上期贷款余额 
	  
	  复借率 = (再次借贷借款人数量 / 总贷款人数量) × 100%‌
	  
	  提前结清率 = 提前结清借据数 / 总借据数

指标计算:
再次借贷借款人数量:当年内有两份借据以上，且第一份借据已结清，再用count计算客户数量

次级类/可疑类/损失类/总贷款余额= 逾期天数 > 90 and 数据日期 = '采集日期' and 贷款发放日期 between 采集日期年初 and 采集日期
					
逾期贷款余额= 从利息逾期和本金逾期取最小的那个值，然后当前日期减去这个最小值 > 90 and 数据日期 = '采集日期' and 贷款发放日期 between 采集日期年初 and 采集日期 and 信贷业务品种号 like '0202%'
										  
期末不良贷款：逾期天数 > 120 and 数据日期 = '采集日期' and 贷款发放日期 between 采集日期年初 and 采集日期 and 信贷业务品种号 like '0202%'

期初不良贷款：逾期天数 > 120 and 数据日期 = last_day(add_months('采集日期',-1))+1 and 贷款发放日期 between 采集日期年初 and 采集日期 and 信贷业务品种号 like '0202%'

核销不良贷款：数据日期 = '采集日期' and 贷款发放日期 between 采集日期年初 and 采集日期 and 信贷业务品种号 like '0202%'，从贷款核销信息表筛选出本期的核销收回本金，并进行聚合

期初贷款余额：数据日期 = last_day(add_months('采集日期',-1))+1 and 贷款发放日期 between 采集日期年初 and 采集日期 and 信贷业务品种号 like '0202%'


事实表
贷款账户还款处理明细表（Loan_Account_Fact） from 贷款账户还款处理表
1.Loan_Account_ID （贷款账户ID，主键）
2.贷款借据号 (主键)
3.Customer_ID （客户ID，外键到客户维度表）
4.Loan_Product_ID （贷款产品ID，外键到产品维度表）
5.Loan_Start_Date （贷款开始日期）
6.Loan_End_Date （贷款结束日期）
7.Total_Loan_Amount （总贷款金额）
8.贷款余额
9.当前期数

贷款交易明细表（Loan_Transaction_Fact）
1.Transaction_ID （交易ID，主键）
2.Loan_Account_ID （贷款账户ID，外键）
3.Transaction_Date （交易日期，外键到时间维度表）
4.Transaction_Type （交易类型，还款、提款）
5.Transaction_Amount （交易金额）
6.Outstanding_Balance （未偿还余额）

贷款账户借据明细表
1.Loan_Account_ID （贷款账户ID）
2.贷款借据号 (主键)
3.客户号
4.贷款合同号
5.总期数
6.贷款金额
7.贷款余额
8.当前已还款期数
9.五级分类
10.逾期本金金额
11.逾期期数
12.逾期天数
13.贷款损失准备金额

核销信息明细表
1.贷款借据号 主键
2.贷款账号 外键
3.核销贷款余额
4.核销贷款拖欠利息
5.核销贷款拖欠复利
6.核销日期


维度表
客户维度表（Customer_Dimension）from 客户信息表
1.Customer_ID （客户ID，主键）
2.Customer_Name （客户姓名）
3.Date_of_Birth （出生日期）
4.Occupation （职业）
5.Contact_Details （联系方式）

贷款合同维度表 from 贷款合同表
1.合同编号 （主键）
2.贷款账号
3.贷款种类
4.贷款期限

产品维度表（Product_Dimension）
1.Loan_Product_ID （贷款产品ID，主键）
2.Product_Name （产品名称）
3.Interest_Rate_Type （利率类型）
4.Loan_Term （贷款期限）

担保物维度表
1.合同编号
2.担保物编号
3.担保物类型
4.评估值
5.抵押率
6.担保物名称

抵押物维度表
1.合同编号
2.抵押人
3.抵押物类型
4.交易价值

逾期维度表
1.贷款借据号
2.逾期天数
3.逾期期数


信贷业务流程：
贷款五级分类
正常，关注，次级，可疑，损失
正常贷款核心定义：债务人能够履行合同，没有足够理由怀疑债务人不能按时足额偿还债务。
关注贷款核心定义：1<贷款逾期天数<90，尽管债务人目前有能力偿还债务，但存在一些可能对偿还产生不利影响的因素，尽管债务人目前有能力偿还债务，但存在一些可能对偿还产生不利影响的因素。
次级贷款核心定义：91<贷款逾期天数<120，债务人的还款能力出现明显问题，完全依靠其正常营业收入无法足额偿还贷款本息，即使执行担保，也可能会造成一定损失。
可疑贷款核心定义：121<贷款逾期天数<180，借款人无法足额偿还贷款本息，即使执行担保，也肯定要造成较大损失。
损失贷款核心定义：贷款逾期天数>180，在采取所有可能的措施或一切必要的法律程序之后，本息仍然无法收回，或只能收回极少部分。

贷前: 事实表 维度表
贷款申请 贷款申请流水表、贷款申请明细 客户信息、地域、年龄、申请渠道
贷款核查 核查明细表、核查流水表 还款能力、教育背景、就业
征信上报 征信上报流水表
贷款审批 贷款审批流水表、流水明细表
贷款开户 开户审批流水、明细表 产品
贷款放贷 放款流水、明细表 金额

贷中：
利率调整 流水表、明细表、客户申请表 客户申请维度
贷款还款 流水表、明细表 客户、产品、还款渠道
贷款延期 贷款延期申请表、流水表 产品、户口

贷后：
贷后催收 贷后催收流水、明细 客户信息、地域
贷后核销 贷后核销流水、明细 客户、产品、征信

信贷业务
信贷业务从业务流转来看，分为贷前、贷中、贷后。不同的环节，继续拆分:
贷前又分为:
贷款申请、贷款审查、贷款审批、贷款开户、贷款发放。贷中细分为贷款计息、贷款利率调整、贷款还款、贷款展期、贷款形态转移。贷后细分为催收/委外、贷款核销、结清销户。
在日常工作当中，信贷业务的拆分更加详细，细致到了功能模块，站在个人的角度，信贷业务流程可以拆分为以下节点产品设计、产品营销、获取用户、账号注册、信息收集、授信审批、额度审批、账户冻结/解冻、借款提现、提现审批、发放贷款、贷款计息、放款展示、征信上报、贷款还款、贷中监控、提额/降额、展期/缩期、催收/委外、贷款核销、结清销户。


大数据风控 - 报表监控体系
1. 贷前监控
2. 贷中监控
3. 贷后监控
在信贷反欺诈领域，报表监控也是风险管理过程中非常重要的工作内容，由于数据量大、数据维度多且涉及到多个环节，报表监控能够监控和分析客户在各个流程的状态，以便于策略人员了解资产质量、产品分布、模型效果等。以下为总结的一些可以监控的数据维度。
1. 贷前监控
贷前报表可以从业务、数据、模型等层面进行监控。
业务层面：可以监控进件量、准入策略通过量(率)[如:人行征信]、内部风控策略通过量(率)、报单量(率)、审批通过量(率)、放款量(率)，放款金额、件均金额等，公司每日的运营情况一目了然，如下例：数据层面：可以监测变量的缺失率、异常率、变量分布等，如果从系统层考虑也可以监测数据调用失败的情况。
模型层面：主要监测现有风控策略的情况，可以监控总体风控策略通过率、各规则命中情况、评分卡可以监控 PSI。可以观测每天各规则命中情况及各规则每天的命中情况，可以根据策略的运行效果以及公司政策对策略进行调整。

2. 贷中监控以下为贷中监控常用的监控指标：
Vintage 就是帐龄，Flow Rate 滚动率，CPI 账户逾期率，API 金额逾期率，FSTQPD 就是首逾、首二逾、首三逾、首四逾
整体情况：
逾期情况：
资产质量情况：
流转率：可以用于评估催收的绩效
假设从 M2->M3 的借款人，流转率为 40%，说明逾期60 天以上的到逾期 90 天这个阶段的借款有 60%都回款了，一般来说，逾期期数越高，流转率越高，回款率越低

3. 贷后监控
贷后可以监控分析逾期客户的特征和催收情况
贷后特征监控：可以后续的模型迭代优化提供数据支撑，可以从以下特征进行分析监控
客户特征： 性别、年龄、婚姻状况、学历等
地域特征： 省份、大区、城市等
放款时段： 季节、月份、工作日、非工作日、节假日等
产品特征： 产品种类、贷款期数等
贷款标的物特征等（车贷、房贷可以参考）
催收情况：
可以区分 M1、M2、M3+监控每日催回笔数和催回金额的情况
可以监测催收人员拨打电话数、催收电话时长、回款数等，及时对催收策略进行调整和优化，以便提高催收效率。


################################################################################################################################
mapreduce工作原理：首先程序会获取计算所需要的数据，然后将原始数据分割成多个小块，
                  然后这些数据块被分配到集群中的不同节点里去执行mapper任务，
                  通过mapper函数对数据进行键值对映射，将原始数据转换为中间结果  #（mapper可以处理任何用户自定义的逻辑，例如文本分割，计数，过滤等）
                  这些中间结果通过哈希算法打上标记，并写入到mapper的内存缓冲区中，
                  如果写入的数据超过内存缓冲区阈值，数据就会溢写到磁盘上，在溢写的过程中会对key进行排序和归并，
                  如果中间结果比较大，就会形成多个溢写文件，这些溢写文件里中间结果会根据之前打上的标记汇总成分区文件
                  然后这些分区文件被发送到reducer任务中，reducer会接收这些文件里键值对的集合，并对每个键对应的值进行聚合操作  #（如求和，最大值，最小值等）
                  最后生成最终结果

表内利息：指正常的贷款利息收入    表外利息: 指逾期贷款的利息收入     税务处理有差异

怎么建模：事实表从ER模型根据业务事件（采用的就是业务库的ER模型），（有一个实体 比如一次新的交易，下面有他的属性，就是椭圆形关注的），
          用powerdesigner生成逻辑模型，然后落实到物理模型，生成数据字典

下游：报表分析，做风控模型部门，报送系统，智能营销等等；
上游：个贷业务技术开发团队，其它系统的数仓

统一授信和分销授信：

分桶：单个表数据量过大，加速join查询速度，数据抽样

时间戳与时间的相互转换：时间戳转时间 from_unixtime: select from_unixtime('20240926 12:00:00', 'yyyy-MM-dd HH:mm:ss');
                       时间转时间戳 unix_timestamp: select unix_timestamp('20240926', 'yyyyMMdd');

distinct 与group by区别和效率：distinct需要对比表中每一行，而group by 是分组处理

(not) in 和 （not）exists的区别和效率: exists: 先查询外表，对外表做loop查询，再对内表查询
                                     in：把外表跟内表进行hash连接，先查询内表返回结果集，再去跟外表去作比较
                                     内外表数据量差不多时，in和exists性能相当；
                                     内外表数据量相差大时，遵循小表驱动大表的原则，外表数据量较小时，用exists；内表数据量较小时，用in；

分区索引和全局索引的区别，分别在什么情况使用 

子查询与join区别：子查询是select语句嵌套另一个select语句
                 join是两个或多个表通过关联字段进行关联
                 join比子查询更高效，因为join不需要多次访问数据库里的表

指标拆解是什么概念：

如何解决数据口径不一致问题，如何避免：
原因：1.业务视角和技术视角的差异
     2.不同部门使用相同指标的不同定义
     3.多样数据产品
处理方法：1.统一标准和规范：与业务进行研讨得出一致的数据标准
         2.构建数据词典，像码表等：明确各业务线的指标名称和内涵，明确指标的计算方式，数据来源，统计维度等
         3.利用数据产品和平台，搭建指标管理平台来保证数据口径统一（最有效）
指标管理平台作用举例：逾期天数————10月1号为还款日，11月1日未还是否逾期，每个业务部门理解不一样，指标管理平台可统一指标口径
                      再贷金额：运营部门与风控部门上报的再贷金额一致，运营部门报1000亿，风控部门报1200亿，该指标对两个部门的利益不一致，需要拉上更上级的架构师或领导来作判定
                      唯一性稽核能力：在指标管理平台先录入的指标从 1.维度，2.业务限定，3.统计周期，4.参与计算的原子指标 来判断指标是否重复
    
埋点数据？

A表3条，B表5条，两表内连接数据量：0-15，左连接数据量：3-15

保证数据质量：实现数据血源追踪，
             统一数据口径，
             数据清洗转换，
             做数据校验自查（数据的唯一性，完整性（行数检查，空值检查），精确性，合法性（长度检查，格式检查，值域检查），时效性），
             检查数据依赖：表加上某些字段后，生产环境里需要检查是否有上游表的权限，没有则报错
             SIT测试， UAT测试，
             做好版本回退计划

with临时表的优缺点：优点：1.一次分析，多次使用
                         2.增加sql的易读性
                    缺点：1.with as临时表不支持索引


同一客户可在同一银行开设多个贷款账号吗： 可以但共享额度

mapping的设计过程：上下层的表英文名，中文名，字段英文名，字段中文名，字段类型，字段长度，加工规则，默认值，映射说明，映射备注，作业所属系统，处理频率 等等；

map卡在66%-67%不动怎么处理：

目标表报主键冲突：

上游的数据如何保证正确：

票据和保函了解？票据：银行票据是指由银行签发或由银行承担付款的票据。主要包括： 银行本票、银行汇票、银行签发的支票等
               保函： 指银行向第三方开立的一种书面信用担保凭证，与担保业务相关

hive八位日期字符串转化为DATE类型：SELECT from_unixtime(unix_timestamp('20230101', 'yyyyMMdd'), 'yyyy-MM-dd') AS formatted_date;

为什么开启hive本地模式，好处，设置哪个参数？

建立宽表的原则：宽表的好处：1.减少了重复工作和逻辑复杂度，查询性能的提升，从而使开发效率得到提升
                          2.统一指标口径
                          3.数据质量稳定，因为统一了数据粒度和指标口径，基于大宽表的开发能有效规避对业务理解的不透彻和逻辑错误，减少数据质量问题

               宽表的缺点：1.性能低：计算复杂，依赖多，依赖链路长
                          2.稳定性低：依赖多，短板理论
                          3.开发难度大、维护成本高：修改得在几千行sql代码里去加逻辑

               如何建立宽表：1.确定主键id：如借据id
                            2.确定对象属性：如借据表相关信息，客户姓名，贷款金额，贷款时间等
                            3.确定对象与对象之间的关系：借据所关联的合同，合同有什么相关信息等
                            4.确定对象的指标：该借据贷款余额，逾期天数，五级分类等

                原则：1.主次分离：冗余的信息越来越多，主题就被弱化，就得进行宽表拆分
                      2.冷热分离：多个表用前150个字段数据，一两个表用后50个字段数据，这种情况就得做冷热拆分
                      3.稳定与不稳定分离：如果宽表依赖外部大表补维度从而导致计算经常延迟，可考虑关联后退，即在报表层做关联，从而弱化宽表与不稳定依赖表的耦合

怎么对接：jira把任务发给我们团队，然后项目经理分配任务，然后自己找相应的业务沟通，根据任务要求做需求分析说明书，然后开发

连续登录问题：select id, count(1) as login_duration from 
             (select id, date, rn, date_sub(date, rn) as sub_date from 
             (select id, date, row_number() over(partition by id order by date) as rn from <table> group by id, date) a) b 
             group by id, sub_date 
             having login_duration >= 3;

数仓中表的命名规范，说一张表明（英文）：

不用拉链表的情况下保留历史数据：使用分区，把新数据加载到新分区中

hive比较两数值大小：select greatest(-1, 0, 5, 8, null) --null
                   select greatest(-1, 0, 5, 'a') --null #某column字段类型是string，而其他column字段是int/double/float,返回null
                   select least(-1, 0, 5, 8) -- -1


维度退化场景：1.定义：退化维度通常是指在事实表中直接包含维度表的某些属性
             2.目的：退化维度的主要目的是为了优化查询性能
             3.例子：如借据ID，本属于维度的一种，退化到事实表中让查询变得高效，减少的复杂度，让数据更直观易懂

如何保证数据一致性：与业务人员作研讨，确定数据口径

数据粒度：表中每行数据表示什么

数据中台功能： 数据规范：基础数据源，数据调研（数仓分层，业务流程设计，主题设计），数据标准（编码标准，命名标准，数据元标准，命名词典，度量单位），数据模型（逻辑模型，物理模型），数据指标（指标等级，原子指标，派生指标，衍生指标）
              项目开发：离线开发（批量同步，离线作业，脚本管理，函数管理，数据查询，资源管理），实时开发，运维管理（运维概览，批量同步实例，离线作业实例，实时作业实例），预警管理（预警配置，预警记录），数据质量（质量稽核，质量监控，质量报告）
              数据资产：资产目录（概览，资产搜索），元数据管理（元数据采集，元数据查询，元数据分析，任务监控），数据探索（概览，数据表管理，数据查询，数据地图）
              数据安全：数据密级（密级定义，用户级别，识别规则），数据脱敏，敏感数据，数据权限（数据访问权限，访问分组，访问授权，数据开发权限）
              数据服务：授权管理，类目管理，数据资源（元模型管理，元数据管理，任务监控，api管理），服务资源，资源市场


如何评判一个数仓模型的好坏：1.数据准确性：和原始数据作对比和校验
                           2.数据质量：数据完整性，一致性，准确性，可靠性，时效性，通过数据清洗，校验，补全等
                           3.数据建模：数仓模型建模考虑到数据维度，度量，关系等，需要关注模型的可扩展性，可维护性，易用性等
                           4.数据集成：数据来自多个不同的源，需要关注数据集成的效率，稳定性，准确性等
                           5.数据分析：数仓的目的，可通过选用该模型的业务多少，业务下游业务人员的评价来评判
                           6.成本效益：计算成本，存储成本，开发维护成本

数据少了：数据漂移
数据发散：关联字段值不唯一，关联条件的粒度没达到最细

数仓的规范： 1.数仓架构规范：1.1.分层架构规范：明确每层有什么作用，数据域按照什么原则进行划分，业务过程应该挂靠哪个数据域
                           1.2.层间调用规范：层间不能反向依赖，不能跨层调用
            2.数仓模型规范：2.1.模型设计的规范：每层该如何设计。如信贷的dwd层应该做哪些数据的清洗，数据标准化，退化高频维度，根据总线矩阵的指导和业务调研来确定这些规范
                           2.2.模型命名规范：ods,dwd,dws具体是啥名字
                           2.3.词根管理：如生产会统一用prod这个词根
                           2.4.指标体系规范：每个指标构成是什么，对指标怎么拆解，哪些指标放在哪一层
            3.数仓流程规范：3.1.需要承接规范：系统把任务发给项目经理，然后项目经理分配任务，然后自己找相应的业务沟通，根据任务要求做需求分析说明书，然后开发
                           3.2.运维机制规范：夜间任务跑失败告警机制，每5分钟发一次告警短信到手机上，不回复30分钟后，升级告警范围（短信发到你上级领导），运维首要原则恢复生产数据
                           3.3.上线规范：充足的测试，数据核验，代码审核
            4.数仓管理规范：4.1.存储管理规范：每层数据保留多久
                           4.2.数据安全规范：哪些数据需要加密，脱敏等，数据权限的把控
            5.数据开发规范：5.1.ETL规范
                           5.2.代码风格规范

为什么要做数据域的划分：帮助我们管理数据的模型，支撑数据模型的建设，一个合理的数据域划分可以提升数据查询的效率，减少数据冗余，避免重复开发的问题

hive行转列（多行转一行）：concat_ws(',', collect_list(列名))
例子：A表
name      course
cht       english
cht       maths
cht       linux
cht       shell
cht       python

行转列语句：select name, concat_ws(',', collect_list(course)) as course_list from A group by name;
其中collect_list不作去重，collect_set会去重，都是hive里的聚合函数，需要结合group by使用

hive列转行（一行转多行）：lateral view explode(split(列名, 分隔符)) 临时表名 as 新列名
1.B表原数据形式
movie   category
教父	黑帮,警匪,心理,剧情
侏罗纪公园	动作,科幻,剧情,灾难
战狼	战争,动作,灾难

2.需求形式：
movie   category_name
教父	黑帮
教父	警匪
教父	心理
教父	剧情
侏罗纪公园	动作
侏罗纪公园	科幻
侏罗纪公园	剧情
侏罗纪公园	灾难
战狼	战争
战狼	动作
战狼	灾难

列转行语句：select movie,category_name from B lateral view explode(split(category, ',')) B_tmp as category_name;
说明：lateral view 侧视图将 explode 函数里的炸裂开的数据与原来同一行的 movie 字段数据分别对应上，生成 B_tmp 这个只有一个字段的虚拟表，再给这一列取新的列名 category_name（需要与 select 里的列名对应上）
      explode 函数值接收map，array类型的数据，split 切割函数能把输入的数据转化为数组，用法为 split(列名, 分隔符)

需求分析调研流程：从jira上接到业务给我们team的需求，然后整个团队跟业务去开会讨论这些需要的可行性，比如上游的业务系统没有这些数据，那就没法做；、
                 然后评估下这需求开发大概所需要的时间，需要用到的表，字段，是否能复用已有的表，如果能，那需要从数仓的哪一层开始进行开发等，然后跟业务确认数据的粒度，指标计算的公式，指标开发的口径，比如说提前结清率这个指标到底是用户口径还是金额口径，这些都需要跟业务去逐一确认
                 最后是双方确认一个排期时间，因为基本上任务不是一个做完才去接另外一个，都是不定期给的，需要排期来完成

事务型事实表：每一行记录一次业务事件，像订单，交易等，每一列都记录相应的度量值，如金额，数量等，适用于分析一个业务事件的细节；
周期型快照事实表：相比于事务型事实表，周期性快照事实表记录的是某段时间内的累计度量，而不是某次业务事件，如每月末，每周末的销售总额等，适用于分析业务的趋势和周期变化；
累计型快照事实表：累计型快照事实表用于记录某一个业务过程，通常由多个时间点组成，每个时间点分别记录相对应的业务阶段，所以累计型快照事实表包含一个时间键和状态指标键，累计型快照事实表常会用在分析业务流程的效率和进展方面；

hive优化：
1.小数据量时使用本地模式：
  (1)set hive.exec.mode.local.auto=true; //开启本地mr
     用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化。
  (2)set hive.exec.mode.local.auto.inputbytes.max=50000000;
     设置local mr的最大输入数据量，当输入数据量小于这个值时采用local mr的方式，默认为134217728，即128M
  (3)set hive.exec.mode.local.auto.input.files.max=10; 
     设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4
2.减少参与查询的数据量，先筛选再计算或连接；
3.使用mapjoin，如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜
  (1)设置自动选择Mapjoin
     set hive.auto.convert.join = true; 默认为true
  (2)大表小表的阀值设置（默认25M一下认为是小表）；
     set hive.mapjoin.smalltable.filesize=25000000;
4.Group By优化，默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。
  （1）开启Map端聚合参数设置,默认为True
       hive.map.aggr = true
  （2）在Map端进行聚合操作的条目数目
       hive.groupby.mapaggr.checkinterval = 100000
  （3）有数据倾斜的时候进行负载均衡（默认是false）
       hive.groupby.skewindata = true
5.Count(Distinct) 去重优化方案(数据倾斜)
  数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换，虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得的。
  select count(distinct id) from bigtable;(优化前)
  select count(id) from (select id from bigtable group by id) a;(优化后)
6.避免迪卡尔基
  尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积。
7.开启并行执行
  set hive.exec.parallel=true; //打开任务并行执行
  set hive.exec.parallel.thread.number=16; //同一个sql允许最大并行度，默认为8。
8.执行计划分析
  通过explain查看sql运行的步骤，逻辑，数据类型，运行顺序等
9.修改表格存储格式
  大表使用ORC 或 Parquet
10.分区，分桶
11.根据表结构合理设置map跟reduce数量

hive任务跑批延迟：看日志，原因：数据量大（上游数据是否异常，数据量是否膨胀，分区裁剪是否失效），小文件多，数据倾斜，爆内存不足引起任务反复重试

explain:
id（重要）：id相同，表从上至下执行，id越大，表越先执行；
select_type（了解）：1.simple：不包含子查询或union等复杂部分；
                     2.primary：任何包含复杂子部分的sql的最外层；
                     3.subquery：select或where里包含的子查询；
                     4.derived：from后面包含的子查询；
                     5.union：若第二个select出现在union之后，则被标记为union，若union包含在from后的子查询中，外层select将被标记为derived；
                     6.union result：从union表获取结果的select；
table（了解）：表，衍生表（子查询作为表）将标记为derived+id作为表名；
type（重要）：从好到坏：system > const > eq_ref > ref > range > index > all
      system：单表单记录 
      const：通过一次索引就找到了，例如：select * from <table> where id = 1; id是主键，且就筛选等于1这行数据
      eq_ref：唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配，常见与主键或唯一索引扫描（等值匹配）
      ref：返回匹配某个单独值的所有行，本质上也是一种索引扫描，但会找到多个匹配的行（等值匹配）
      range：只检索给定范围的行，使用一个索引来选择行，key列显示使用了哪个索引，一般是在在where语句中出现between，in，<，>等语句
      index：覆盖索引，select id,name from <table>，id跟name都是索引，所以查询索引表就能查到所要的所有值，这称为覆盖索引
      all：全表扫描
possible_keys（了解）：可能用到的索引
keys（了解）：实际用到的索引
key_len（了解）：索引使用到的字节数，例如一个utf—8的表里其中一字段为varchar(24)，且不允许为空，则 key_len=24*3+2=74，
         因为utf-8是一个字符3个字节组成，又因为是varchar是可变长度要+2，char则不需要+2，若字段允许为空则再+1，非空则不需要+1
ref（了解）：表示使用到哪些索引；
rows（了解）：根据表统计信息及索引选用情况，大致估算出找到所需要的记录所需要读取的行数；
extra（重要）：额外的信息说明；
              1.useing filesort：仅仅表示没有使用索引的排序，当sql包含 ORDER BY 操作，而且无法利用索引完成排序操作的时候，MySQL Query Optimizer 不得不选择相应的排序算法来实现。数据较少时从内存排序，如果结果集超过了sort_buffer_size大小则从磁盘排序。
              2.Using temporary：要解决查询，MySQL需要创建一个临时表来保存结果。 如果查询包含不同列的GROUP BY和ORDER BY子句，则通常会发生这种情况。官方解释：”为了解决查询，MySQL需要创建一个临时表来容纳结果。
                                 典型情况如查询包含可以按不同情况列出列的GROUP BY和ORDER BY子句时。很明显就是通过where条件一次性检索出来的结果集太大了，内存放不下了，只能通过临时表来辅助处理；
                                 当mysql需要创建临时表时，选择内存临时表还是硬盘临时表取决于参数tmp_table_size和max_heap_table_size，内存临时表的最大容量为tmp_table_size和max_heap_table_size值的最小值，当所需临时表的容量大于两者的最小值时，mysql就会使用硬盘临时表存放数据。



Hello, interviewer. My name is Chen Hengtao. I graduated from Guangzhou Maritime Institute. 
After graduation, I worked as a Linux operation and maintenance engineer for about 2 years and worked as an ETL engineer for about 4 years.
During my time as an ETL engineer, I worked on 3 data warehouse projects. 
My responsibilities in the most recent data warehouse projects included project-related business research, 
design of the overall ETL process, scheduling and monitoring,
cleaning and conversion of data, create shell scripts to handle daily work, and write some documents which project needs.
In addition to doing my job well, I also assisted in the design of data warehouse models and the production of reports. 
I was able to communicate and collaborate with sales staff, architects and other technicians to ensure that project work can be implemented efficiently.


My most recent project was a retail loan data warehouse project outsourced to Bank of Quanzhou.
My main responsibilities in this project was
when the customer sends the requirements to our team in jira, 
we hold a requirements clarification meeting with the relevant sales staff to confirm the data acquisition logic, 
the granularity and caliber of the indicators
We will also discuss how to implement the requirements, conduct feasibility analysis, which business system tables and fields to use, 
whether the existing tables in the data warehouse can be reused etc.
Our data warehouse is designed based on the star model, 
and each data developer loads data into DW layer and DM layer according to the secondary subject domain which assigned to him
In this project, I am mainly responsible for the theme of risk control domain. 
From the DWD layer, I should determine cleaning rules according to the caliber in the Indicator System Management Manual,
and Remove duplicates, null values, and outliers from ODS layer data, and also unify data types and data formats, etc.
Then load the cleaned data into each fact table on DWD layer,
then aggregate the fact tables and dimension tables which related to the risk control domain theme into a wide table 
to provide unified basic data and indicators for the DM layer,
finally, further aggregate the basic data from the dws layer according to the needs, and then output the final indicators to each DM table, 
After completing the hive sql code, 
I will conduct a self-test to check the uniqueness, integrity, legality, accuracy, and timeliness of the data to ensure the quality of the data to a certain extent. 
If there is no problem, encapsulate the hive sql code into shell scripts,
and hand it over to the test engineer to do SIT test and UAT test. 
After the test passes, I will draft the online document and hand it over to the DBA to arrange those new tables online;


巴塞尔协议：银行风险管理手册。

巴塞尔协议一二三的区别
巴塞尔协议I、II、III之间的主要区别体现在监管目标、‌风险覆盖范围、‌资本充足率要求以及监管框架的构成上。这些区别反映了银行监管标准的逐步发展和完善，以应对日益复杂的金融风险。

巴塞尔协议I
时间：1988年7月颁布。
主要内容就是银行的放贷规模必须以自有的资本金想匹配想挂钩，银行风险管理手册。
核心内容：确立了以风险为基础的资本充足率标准，包括资本定义、风险加权资产（RWA）的计量方法，以及最低资本充足率要求（核心资本充足率4%，总资本充足率8%）。
风险覆盖：主要关注信用风险，对市场风险和操作风险的覆盖较少。

巴塞尔协议II
时间：2004年6月正式实施。
核心内容：构建了以“最低资本要求、监管部门的监督检查和市场约束”为三大支柱的监管框架，扩大了风险计量范围，包括信用风险、市场风险和操作风险。
风险覆盖：引入了内部评级法，允许银行基于内部模型计量各类风险，并强调基于全面风险评估进行资本管理。
资本充足率=（资本-扣除项）/风险加权资产，要求资本充足率8%，核心资本充足率4%

巴塞尔协议III
时间：2010年12月发布，2013年1月1日起实施。
核心内容：在巴塞尔协议II的基础上，进一步提高了资本充足率要求，引入了反周期超额资本，并严格了资本扣除限制，以应对金融危机后的银行监管需求。
风险覆盖：继续强化对信用风险、市场风险和操作风险的监管，并提出了更严格的资本和流动性要求。

巴III规定核心一级资本充足率为4.5%，一级资本充足率为6%，总资本充足率为8%

总结
巴塞尔协议的这三个版本代表了银行监管标准的逐步发展和完善。从最初的关注信用风险到后来的全面风险管理，再到应对金融危机后的进一步强化，巴塞尔协议体现了对银行监管的不断适应和改进。这些协议共同构成了全球银行业监管的基础，确保了银行的稳健运营和金融系统的稳定
